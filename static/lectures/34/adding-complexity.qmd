---
title: "Adding Complexity"
institute: "STAT 20 UC Berkeley"
logo: "images/stat20-hex.png"
format: 
  revealjs:
    theme: "../../assets/stat20.scss"
    highlight-style: breezedark
    slide-number: true
    incremental: true
    menu: false
    title-slide-attributes:
      data-background-image: "images/hex-background.png"
      data-background-size: cover  
    progress: false
execute:
  freeze: auto
---

```{r include = FALSE}
library(tidyverse)
library(patchwork)
library(infer)
knitr::opts_chunk$set(echo = TRUE,
                      fig.align = "center")
set.seed(80)
```

## Agenda

1. Common mathematical assumptions of the linear model
  - Residual analysis
2. Multiple Regression
  - Example: shipping books
  - Adding complexity: Two intercepts
3. Extending the Model
  - Interaction terms

## {background-image="images/model-assumptions-1.jpg" background-size="contain"}

## {background-image="images/model-assumptions-2.jpg" background-size="contain"}

## {background-image="images/model-assumptions-3.jpg" background-size="contain"}

## {background-image="images/model-assumptions-4.jpg" background-size="contain"}


## {}

<br>
<br>
<br>

[That is simple linear regression. Let's begin to add some complexity.]{.smalladage}


## Example: shipping books

```{r out.width=480, echo = FALSE}
knitr::include_graphics("images/pile-of-books.jpg")
```

When you buy a book off of Amazon, you get a quote for how much it
costs to ship. This is based on the weight of the book. If you
didn't know the weight a book, what other characteristics of it
could you measure to help predict weight?

```{r getdata, echo = FALSE, message=FALSE}
library(DAAG)
data(allbacks)
books <- allbacks[, c(3, 1, 4)] %>%
  tibble()
```


## The data

Consider the following data set, a simple random sample of books from Amazon's catalog where the weight of the books is known.

```{r booksdata, eval = FALSE}
books %>%
  select(weight, volume)
```

. . .

```{r ref.label = "booksdata", echo = FALSE}
```


## Shipping books visualized

. . .

```{r plotallbacks, echo = FALSE, fig.width=9, fig.height = 6.7}
p1 <- ggplot(books, aes(x = volume, y = weight)) +
  geom_point(size = 3) +
  theme_bw(base_size = 18)
p1
```


## Shipping books visualized, cont.

```{r fitm1, echo = FALSE}
m1 <- lm(weight ~ volume, data = books)
```

```{r plotallbackswline, fig.width=9, fig.height = 6.7, echo = FALSE}
p1 + 
  geom_abline(intercept = m1$coef[1], slope = m1$coef[2],
              col = "blue", lwd = 2)
```


## Fitting the linear model

. . .

```{r lm, eval = FALSE}
m1 <- lm(weight ~ volume, data = books)
summary(m1)
```

. . .

```{r ref.label = "lm", echo = FALSE}
m1 <- lm(weight ~ volume, data = books)
summary(m1)
```


## {}

**Question 1**: What is the equation for the line?

. . .

$$ \hat{y} = 107.7 + 0.708 x $$

$$ \widehat{weight} = 107.7 + 0.708 volume $$


## {}

**Question 2**: Does this appear to be a reasonable setting in which to apply linear regression for inference?

. . .

What is the population we wish to draw inferences on?

> All books sold on Amazon

. . .

We need to consider:

::: nonincremental
1. Linear trend
2. Independent observations
3. Normal residuals
4. Equal variance
:::


## Residual Plot

. . .

```{r resplot, fig.width=9, fig.height = 6.7, echo = FALSE}
ggplot(m1, aes(x = .fitted, y = .resid)) +
  geom_point(size = 3) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals") +
  theme_bw(base_size = 18)
```


## Histogram of Residuals

. . .

```{r resplot2, warning=FALSE, fig.width=9, fig.height = 6.7, echo = FALSE}
ggplot(m1, aes(x = .resid)) +
  geom_histogram() +
  theme_bw(base_size = 18) +
  labs(x = "Residuals")
```


## {}

**Question 2**: Does this appear to be a reasonable setting in which to apply linear regression for inference?

. . .

We need to consider:

::: nonincremental
1. Linear trend: _Looks reasonable_
2. Independent observations _Seems reasonable_
3. Normal residuals _Questionable_
4. Equal variance _Looks reasonable_
:::

. . .

> We should be skeptical of the accuracy of our p-values.


## {}

**Question 3**: Is volume a significant predictor?

. . .

```{r sumtable}
summary(m1)
```

. . .

**Question 4**: How much of the variation in weight is explained by the model?


## Multiple Regression

. . .

Allows us create a model to explain one $numerical$ variable, the response, as a linear function of many explanatory variables that can be both $numerical$ and
$categorical$.

. . .

<br>

We posit a true model (here with a normal errors assumption):

$$ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p + \epsilon; \quad \epsilon \sim N(0, \sigma^2) $$

We use the data to estimate our fitted model:

$$ \hat{y} = b_0 + b_1 x_1 + b_2 x_2 + \ldots + b_p x_p $$


## Estimating $\beta_0, \beta_1$ etc.

. . .

In least-squares regression, we're still finding the estimates that minimize
the sum of squared residuals.

$$ e_i = y_i - \hat{y}_i $$

$$ RSS = \sum_{i = 1}^n (y_i - \hat{y}_i)^2 $$

And yes, they have a closed-form solution.

$$ \mathbf{b} = (X'X)^{-1}X'Y $$

. . .

In R:

```{r eval = FALSE}
lm(Y ~ X1 + X2 + ... + Xp, data = mydata)
```


## Example: shipping books

. . .

```{r df, eval = FALSE}
books
```

. . .

```{r ref.label = "df", echo = FALSE}
```


## Example: shipping books

```{r plotcolors, echo = FALSE}
p2 <- ggplot(books, aes(x = volume,
                        y = weight, 
                        color = cover)) +
  geom_point(size = 3) +
  theme_bw(base_size = 18)
p2
```


## Example: shipping books

```{r}
m2 <- lm(weight ~ volume + cover, data = books)
summary(m2)
```


## How do we interpret these estimates?

Think about the geometry of the model.


## {background-image="images/dummy-var.jpg" background-size="contain"}

## {background-image="images/ancova-geometry-1.jpg" background-size="contain"}


## Example: shipping books

. . .

```{r echo = FALSE}
p3 <- p2 +
  geom_abline(intercept = m2$coef[1], slope = m2$coef[2], col = 2) +
  geom_abline(intercept = m2$coef[1] + m2$coef[3], slope = m2$coef[2], col = 4)
p3
```


## MLR slope interpretation

. . .

The slope corresponding to the dummy variable tells us:

- How much vertical separation there is between our lines
- How much `weight` is expected to increase if `cover` goes
from 0 to 1 and `volume` is left unchanged.

. . .

Each $b_i$ tells you how much you expect the $Y$ to change when you change the
$X_i$, while **holding all other variables constant**.


## {}

```{r}
summary(m2)
```

- Is the difference between cover types significant?
- How much of the variation in weight is explained by a model containing both
volume and cover?


## Mathematical CIs

. . .

```{r}
coef(summary(m2))
qt(.025, df = nrow(books) - 3)
```

. . .

::: poll
Which of the following represents the appropriate 95% CI for `coverpb`?

::: nonincremental
A. $197 \pm 1.96 \times 59.19$
B. $-184 \pm 2.18 \times 40.5$
C. $-184 \pm -4.55 \times 40.5$
:::
:::

```{r echo = FALSE}
countdown::countdown(1, font_size = "1.5em")
```


## {}

<center>
<iframe src="https://embed.polleverywhere.com/multiple_choice_polls/bUAlI8vXMBnDvqYwsFxes?controls=none&short_poll=true" width="800px" height="600px"></iframe>
</center>


## Mathematical CIs in R

:::: columns
::: {.column width="47%"}

By hand.

```{r}
LB <- coef(m2)[3] + 
  qt(.025, nrow(books)-3) * 40.5
UB <- coef(m2)[3] - 
  qt(.025, nrow(books)-3) * 40.5
c(LB, UB)
```

:::

::: {.column width="6%"}
:::

::: {.column width="47%"}

With `confint()`.

```{r cis}
confint(m2)
```
:::
::::


## {}

::: poll
Take a moment to sketch out the infer pipeline that will result in a collection of 500 bootstrapped slopes that represent slopes that we might have observed had we drawn a different random sample from the population.

Turn to a neighbor and discuss your pipeline. I will ask for a pair to share.
:::

. . .

```{r}
books
```


```{r echo = FALSE}
countdown::countdown(minutes = 2, warn_when = 20, font_size="1.5em")
```


## Bootstrap CIs in R {auto-animate=true}

```{r inf1, eval = FALSE}
books %>%
  specify(weight ~ volume + cover) %>%
  generate(reps = 500, type = "bootstrap") %>%
  calculate(slope)
```

. . .

```{r ref.label = "inf1", echo = FALSE, error = TRUE}
```


## Bootstrap CIs in R {auto-animate=true}

```{r inf2, eval = FALSE}
#| code-line-numbers: "4"
books %>%
  specify(weight ~ volume + cover) %>%
  generate(reps = 500, type = "bootstrap") %>%
  fit()
```

. . .

```{r ref.label = "inf2", echo = FALSE, error = TRUE}
```


## Bootstrap CIs in R, cont.

. . .

```{r inf3, echo = FALSE, fig.width=12, fig.height=3.5}
boot <- books %>%
  specify(weight ~ volume + cover) %>%
  generate(reps = 500, type = "bootstrap") %>%
  fit()
boot %>%
  ggplot(aes(x = estimate)) +
  geom_histogram(col = "white") +
  facet_wrap(vars(term), 
             nrow = 1,
             scales = "free_x") +
  theme_gray(base_size = 18)
```

. . .

```{r echo = FALSE}
confint(m2)
```

. . .

> With $n=15$, bootstrap intervals are likely inaccurate.


# Extending the model


## Extending the model

. . .

```{r echo = FALSE}
p3
```

> The two cover types have different intercepts. Do they share the same slope?


## Extending the model

Think about the geometry.


## {}

```{r out.width=870, echo = FALSE}
knitr::include_graphics("images/ancova-geometry-2.jpg")
```


## Extending the model

```{r echo = FALSE}
p2 +
  stat_smooth(method = "lm", se = FALSE)
```


## Interaction terms

. . .

```{r}
m3 <- lm(weight ~ volume + cover + volume:cover, 
         data = books)
summary(m3)
```

Do we have evidence that two types of books have different relationships
between volume and weight?


## Take home messages

- There is a statistically significant relationship between volume and weight.
- There is a statistically significant difference in weight between paperback and hardcover books, when controlling for volume.
- There is no strong evidence that the relationship between volume and weight differs between paperbacks and hardbacks.

. . .

This is **inference**, which requires **valid models**.


## {}

Recall the original residual plots for the simple model.

```{r, fig.width=12, fig.height = 5, echo = FALSE}
p1 <- ggplot(m1, aes(x = .fitted, y = .resid)) +
  geom_point(size = 3) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals") +
  theme_bw(base_size = 18)
p2 <- ggplot(m1, aes(x = .resid)) +
  geom_histogram() +
  theme_bw(base_size = 18) +
  labs(x = "Residuals")
library(patchwork)
p1 + p2
```


## {}

Residual plots for the two-intercept model.

```{r, fig.width=12, fig.height = 5, echo = FALSE}
p1 <- ggplot(m2, aes(x = .fitted, y = .resid)) +
  geom_point(size = 3) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals") +
  theme_bw(base_size = 18)
p2 <- ggplot(m2, aes(x = .resid)) +
  geom_histogram() +
  theme_bw(base_size = 18) +
  labs(x = "Residuals")
library(patchwork)
p1 + p2
```


## {}

```{r echo = FALSE}
p3
```

<center>
`r emo::ji("ok_hand")`
</center>

